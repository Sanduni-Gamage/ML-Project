{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5eed7fe9",
   "metadata": {},
   "source": [
    "## Project Overview\n",
    "This project implements malware detection using both classical machine learning models (Logistic Regression, SVM, Random Forest, XGBoost) and advanced models (TabTransformer, TabNet, FT-Transformer, and Graph Neural Networks)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4109f62e",
   "metadata": {},
   "source": [
    "## Group Details  \n",
    "\n",
    "-Sanduni Kanapeddala Gamage- Student ID: 1598065\n",
    "\n",
    "-Kahandawita Arachchige Arosh Malindra Perera- Student ID: 1579940\n",
    "\n",
    "-Nanayakkarawasam Juliyan Stephan Nalaka De Silva- Student ID: 1585221\n",
    "\n",
    "-Hungampala Ralalage Malaka Prasad- Student ID: 1599986"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb2b8ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import gc\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Kaggle Dataset Loading - Only basic kaggle API (no kagglehub)\n",
    "import sys\n",
    "import subprocess\n",
    "try:\n",
    "    from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "    KAGGLE_AVAILABLE = True\n",
    "except ImportError:\n",
    "    try:\n",
    "        print(\"Installing kaggle dependencies...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"kaggle\"])\n",
    "        from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "        KAGGLE_AVAILABLE = True\n",
    "    except Exception as e:\n",
    "        print(f\"Could not install kaggle API: {e}\")\n",
    "        KAGGLE_AVAILABLE = False\n",
    "\n",
    "# Data processing\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve, classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# Classical ML Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Feature Importance and Explainability\n",
    "import shap\n",
    "from lime import lime_tabular\n",
    "\n",
    "# Advanced ML Models - TabTransformer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "\n",
    "# FT-Transformer (using PyTorch implementation)\n",
    "class FTTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, dim=32, depth=2, heads=4, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Linear(input_dim, dim)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=dim, nhead=heads, dropout=dropout)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=depth)\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.Linear(dim, dim*2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim*2, output_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = x.unsqueeze(1)  # Add sequence dimension\n",
    "        x = self.transformer(x)\n",
    "        x = x.squeeze(1)\n",
    "        return self.mlp_head(x)\n",
    "\n",
    "# Graph Neural Networks dependencies\n",
    "# Checking if GNN functionality is available\n",
    "\"\"\"\"\n",
    "GNN_AVAILABLE = False\n",
    "try:\n",
    "    import torch_geometric\n",
    "    from torch_geometric.nn import GCNConv, GATConv, global_mean_pool\n",
    "    from torch_geometric.data import Data\n",
    "    from torch.utils.data import DataLoader as GraphDataLoader\n",
    "    GNN_AVAILABLE = True\n",
    "    print(\"GNN functionality is available!\")\n",
    "except ImportError:\n",
    "    print(\"torch_geometric is not installed. GNN functionality will not be available.\")\n",
    "\"\"\"\n",
    "# For visualization\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c6a3dfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No cached datasets found. Loading from Kaggle...\n",
      "Downloading dataset agungpambudi/network-malware-detection-connection-analysis to d:\\Unitec\\Sem2\\Machine learning\\ML Project\\DataSet\\kaggle_data...\n",
      "Dataset URL: https://www.kaggle.com/datasets/agungpambudi/network-malware-detection-connection-analysis\n",
      "Download complete!\n",
      "Found 12 CSV files in the Kaggle dataset\n",
      "Download complete!\n",
      "Found 12 CSV files in the Kaggle dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading CSV files:   8%|▊         | 1/12 [00:04<00:47,  4.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded CTU-IoT-Malware-Capture-1-1conn.log.labeled.csv with 1008748 rows and 23 columns\n",
      "Successfully loaded CTU-IoT-Malware-Capture-20-1conn.log.labeled.csv with 3209 rows and 23 columns\n",
      "Successfully loaded CTU-IoT-Malware-Capture-21-1conn.log.labeled.csv with 3286 rows and 23 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading CSV files:  33%|███▎      | 4/12 [00:05<00:08,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded CTU-IoT-Malware-Capture-3-1conn.log.labeled.csv with 156103 rows and 23 columns\n",
      "Successfully loaded CTU-IoT-Malware-Capture-34-1conn.log.labeled.csv with 23145 rows and 23 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading CSV files:  58%|█████▊    | 7/12 [01:03<00:51, 10.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded CTU-IoT-Malware-Capture-35-1conn.log.labeled.csv with 10447787 rows and 23 columns\n",
      "Successfully loaded CTU-IoT-Malware-Capture-42-1conn.log.labeled.csv with 4426 rows and 23 columns\n",
      "Successfully loaded CTU-IoT-Malware-Capture-44-1conn.log.labeled.csv with 237 rows and 23 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading CSV files:  75%|███████▌  | 9/12 [01:21<00:29,  9.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded CTU-IoT-Malware-Capture-48-1conn.log.labeled.csv with 3394338 rows and 23 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading CSV files:  83%|████████▎ | 10/12 [01:36<00:21, 10.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded CTU-IoT-Malware-Capture-60-1conn.log.labeled.csv with 3581028 rows and 23 columns\n",
      "Successfully loaded CTU-IoT-Malware-Capture-8-1conn.log.labeled.csv with 10403 rows and 23 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading CSV files: 100%|██████████| 12/12 [02:25<00:00, 12.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded CTU-IoT-Malware-Capture-9-1conn.log.labeled.csv with 6378293 rows and 23 columns\n",
      "Saving datasets to cache: d:\\Unitec\\Sem2\\Machine learning\\ML Project\\DataSet\\cache\\kaggle_datasets.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets cached successfully\n",
      "\n",
      "Available datasets:\n",
      "- CTU-IoT-Malware-Capture-1-1conn.log.labeled: 1008748 rows, 23 columns\n",
      "- CTU-IoT-Malware-Capture-20-1conn.log.labeled: 3209 rows, 23 columns\n",
      "- CTU-IoT-Malware-Capture-21-1conn.log.labeled: 3286 rows, 23 columns\n",
      "- CTU-IoT-Malware-Capture-3-1conn.log.labeled: 156103 rows, 23 columns\n",
      "- CTU-IoT-Malware-Capture-34-1conn.log.labeled: 23145 rows, 23 columns\n",
      "- CTU-IoT-Malware-Capture-35-1conn.log.labeled: 10447787 rows, 23 columns\n",
      "- CTU-IoT-Malware-Capture-42-1conn.log.labeled: 4426 rows, 23 columns\n",
      "- CTU-IoT-Malware-Capture-44-1conn.log.labeled: 237 rows, 23 columns\n",
      "- CTU-IoT-Malware-Capture-48-1conn.log.labeled: 3394338 rows, 23 columns\n",
      "- CTU-IoT-Malware-Capture-60-1conn.log.labeled: 3581028 rows, 23 columns\n",
      "- CTU-IoT-Malware-Capture-8-1conn.log.labeled: 10403 rows, 23 columns\n",
      "- CTU-IoT-Malware-Capture-9-1conn.log.labeled: 6378293 rows, 23 columns\n"
     ]
    }
   ],
   "source": [
    "# Kaggle data loading function\n",
    "def load_kaggle_datasets(dataset_id=\"agungpambudi/network-malware-detection-connection-analysis\"):\n",
    "    \"\"\"\n",
    "    Load datasets directly from Kaggle using the Kaggle API.\n",
    "    Uses pipe (|) separator and keeps datasets separate.\n",
    "    \n",
    "    Args:\n",
    "        dataset_id: The Kaggle dataset ID to download\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary of DataFrames with dataset names as keys\n",
    "    \"\"\"\n",
    "    # Check if kaggle is installed\n",
    "    try:\n",
    "        import kaggle\n",
    "        from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "    except ImportError:\n",
    "        print(\"Kaggle API is not installed. Installing...\")\n",
    "        try:\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"kaggle\"])\n",
    "            import kaggle\n",
    "            from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "            print(\"Kaggle API installed successfully\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to install Kaggle API: {e}\")\n",
    "            print(\"Please manually install with: pip install kaggle\")\n",
    "            return None\n",
    "    \n",
    "    # Check if kaggle.json exists\n",
    "    import os\n",
    "    kaggle_dir = os.path.join(os.path.expanduser('~'), '.kaggle')\n",
    "    kaggle_json = os.path.join(kaggle_dir, 'kaggle.json')\n",
    "    \n",
    "    if not os.path.exists(kaggle_json):\n",
    "        print(f\"Kaggle API credentials not found at: {kaggle_json}\")\n",
    "        print(\"\\nTo fix Kaggle authentication issues:\")\n",
    "        print(\"1. Create a Kaggle account at https://www.kaggle.com\")\n",
    "        print(\"2. Go to Account -> API -> Create New API Token\")\n",
    "        print(\"3. This will download kaggle.json\")\n",
    "        print(\"4. Place this file in ~/.kaggle/ (Linux/Mac) or C:\\\\Users\\\\<username>\\\\.kaggle\\\\ (Windows)\")\n",
    "        print(f\"   For your system, that's: {kaggle_dir}\")\n",
    "        print(\"5. Run this cell again\")\n",
    "        \n",
    "        # Create the .kaggle directory if it doesn't exist\n",
    "        if not os.path.exists(kaggle_dir):\n",
    "            try:\n",
    "                os.makedirs(kaggle_dir)\n",
    "                print(f\"Created directory: {kaggle_dir}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Could not create directory: {e}\")\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    # Initialize dictionary to store DataFrames\n",
    "    datasets = {}\n",
    "    \n",
    "    # Create a directory in DataSet to store Kaggle files\n",
    "    base_dir = os.path.dirname(os.path.abspath('__file__'))\n",
    "    dataset_dir = os.path.join(base_dir, 'DataSet', 'kaggle_data')\n",
    "    os.makedirs(dataset_dir, exist_ok=True)\n",
    "    \n",
    "    print(f\"Downloading dataset {dataset_id} to {dataset_dir}...\")\n",
    "    \n",
    "    try:\n",
    "        # Initialize and authenticate Kaggle API\n",
    "        api = KaggleApi()\n",
    "        api.authenticate()\n",
    "        \n",
    "        # Download the dataset files\n",
    "        api.dataset_download_files(dataset_id, path=dataset_dir, unzip=True)\n",
    "        print(\"Download complete!\")\n",
    "        \n",
    "        # List all CSV files in the directory\n",
    "        csv_files = [f for f in os.listdir(dataset_dir) if f.endswith('.csv')]\n",
    "        print(f\"Found {len(csv_files)} CSV files in the Kaggle dataset\")\n",
    "        \n",
    "        # Load each CSV file separately with pipe separator\n",
    "        for file in tqdm(csv_files, desc=\"Loading CSV files\"):\n",
    "            file_path = os.path.join(dataset_dir, file)\n",
    "            try:\n",
    "                # Use pipe separator specifically\n",
    "                df = pd.read_csv(file_path, sep='|', low_memory=False)\n",
    "                \n",
    "                # Store the DataFrame in the dictionary with filename as key\n",
    "                key = file.replace('.csv', '')\n",
    "                datasets[key] = df\n",
    "                print(f\"Successfully loaded {file} with {df.shape[0]} rows and {df.shape[1]} columns\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {file}: {e}\")\n",
    "        \n",
    "        # Cache the datasets for future use\n",
    "        cache_dir = os.path.join(base_dir, 'DataSet', 'cache')\n",
    "        os.makedirs(cache_dir, exist_ok=True)\n",
    "        cache_file = os.path.join(cache_dir, 'kaggle_datasets.pkl')\n",
    "        \n",
    "        print(f\"Saving datasets to cache: {cache_file}\")\n",
    "        with open(cache_file, 'wb') as f:\n",
    "            pickle.dump(datasets, f)\n",
    "        print(\"Datasets cached successfully\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error accessing Kaggle API: {e}\")\n",
    "        print(\"\\nTo fix Kaggle authentication issues:\")\n",
    "        print(\"1. Create a Kaggle account at https://www.kaggle.com\")\n",
    "        print(\"2. Go to Account -> API -> Create New API Token\")\n",
    "        print(\"3. This will download kaggle.json\")\n",
    "        print(\"4. Place this file in ~/.kaggle/ (Linux/Mac) or C:\\\\Users\\\\<username>\\\\.kaggle\\\\ (Windows)\")\n",
    "        print(\"5. Run this cell again\")\n",
    "        return None\n",
    "        \n",
    "    if not datasets:\n",
    "        print(\"No datasets were loaded from Kaggle\")\n",
    "        return None\n",
    "        \n",
    "    return datasets\n",
    "\n",
    "# Load datasets from Kaggle or cache\n",
    "try:\n",
    "    # Check for cached datasets first\n",
    "    cache_file = os.path.join(os.path.dirname(os.path.abspath('__file__')), 'DataSet', 'cache', 'kaggle_datasets.pkl')\n",
    "    \n",
    "    if os.path.exists(cache_file):\n",
    "        print(f\"Loading datasets from cache: {cache_file}\")\n",
    "        with open(cache_file, 'rb') as f:\n",
    "            datasets = pickle.load(f)\n",
    "        print(f\"Loaded {len(datasets)} datasets from cache\")\n",
    "    else:\n",
    "        print(\"No cached datasets found. Loading from Kaggle...\")\n",
    "        datasets = load_kaggle_datasets()\n",
    "        \n",
    "    # Display available datasets\n",
    "    if datasets:\n",
    "        print(\"\\nAvailable datasets:\")\n",
    "        for key, df in datasets.items():\n",
    "            print(f\"- {key}: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading datasets: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7207e4b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available datasets:\n",
      "1. CTU-IoT-Malware-Capture-1-1conn.log.labeled\n",
      "2. CTU-IoT-Malware-Capture-20-1conn.log.labeled\n",
      "3. CTU-IoT-Malware-Capture-21-1conn.log.labeled\n",
      "4. CTU-IoT-Malware-Capture-3-1conn.log.labeled\n",
      "5. CTU-IoT-Malware-Capture-34-1conn.log.labeled\n",
      "6. CTU-IoT-Malware-Capture-35-1conn.log.labeled\n",
      "7. CTU-IoT-Malware-Capture-42-1conn.log.labeled\n",
      "8. CTU-IoT-Malware-Capture-44-1conn.log.labeled\n",
      "9. CTU-IoT-Malware-Capture-48-1conn.log.labeled\n",
      "10. CTU-IoT-Malware-Capture-60-1conn.log.labeled\n",
      "11. CTU-IoT-Malware-Capture-8-1conn.log.labeled\n",
      "12. CTU-IoT-Malware-Capture-9-1conn.log.labeled\n",
      "\n",
      "Analyzing first dataset: CTU-IoT-Malware-Capture-1-1conn.log.labeled\n",
      "='='='='='='='='='='='='='='='='='='='='='='='='='='='='='='='='='='='='='='='='=\n",
      "DATASET: CTU-IoT-Malware-Capture-1-1conn.log.labeled\n",
      "='='='='='='='='='='='='='='='='='='='='='='='='='='='='='='='='='='='='='='='='=\n",
      "Shape: 1008748 rows x 23 columns\n",
      "\n",
      "--- COLUMNS ---\n",
      "- ts: float64\n",
      "- uid: object\n",
      "- id.orig_h: object\n",
      "- id.orig_p: float64\n",
      "- id.resp_h: object\n",
      "- id.resp_p: float64\n",
      "- proto: object\n",
      "- service: object\n",
      "- duration: object\n",
      "- orig_bytes: object\n",
      "- resp_bytes: object\n",
      "- conn_state: object\n",
      "- local_orig: object\n",
      "- local_resp: object\n",
      "- missed_bytes: float64\n",
      "- history: object\n",
      "- orig_pkts: float64\n",
      "- orig_ip_bytes: float64\n",
      "- resp_pkts: float64\n",
      "- resp_ip_bytes: float64\n",
      "- tunnel_parents: object\n",
      "- label: object\n",
      "- detailed-label: object\n",
      "\n",
      "--- SAMPLE DATA ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ts</th>\n",
       "      <th>uid</th>\n",
       "      <th>id.orig_h</th>\n",
       "      <th>id.orig_p</th>\n",
       "      <th>id.resp_h</th>\n",
       "      <th>id.resp_p</th>\n",
       "      <th>proto</th>\n",
       "      <th>service</th>\n",
       "      <th>duration</th>\n",
       "      <th>orig_bytes</th>\n",
       "      <th>...</th>\n",
       "      <th>local_resp</th>\n",
       "      <th>missed_bytes</th>\n",
       "      <th>history</th>\n",
       "      <th>orig_pkts</th>\n",
       "      <th>orig_ip_bytes</th>\n",
       "      <th>resp_pkts</th>\n",
       "      <th>resp_ip_bytes</th>\n",
       "      <th>tunnel_parents</th>\n",
       "      <th>label</th>\n",
       "      <th>detailed-label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.525880e+09</td>\n",
       "      <td>CUmrqr4svHuSXJy5z7</td>\n",
       "      <td>192.168.100.103</td>\n",
       "      <td>51524.0</td>\n",
       "      <td>65.127.233.163</td>\n",
       "      <td>23.0</td>\n",
       "      <td>tcp</td>\n",
       "      <td>-</td>\n",
       "      <td>2.999051</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-</td>\n",
       "      <td>0.0</td>\n",
       "      <td>S</td>\n",
       "      <td>3.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-</td>\n",
       "      <td>Malicious</td>\n",
       "      <td>PartOfAHorizontalPortScan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.525880e+09</td>\n",
       "      <td>CH98aB3s1kJeq6SFOc</td>\n",
       "      <td>192.168.100.103</td>\n",
       "      <td>56305.0</td>\n",
       "      <td>63.150.16.171</td>\n",
       "      <td>23.0</td>\n",
       "      <td>tcp</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>...</td>\n",
       "      <td>-</td>\n",
       "      <td>0.0</td>\n",
       "      <td>S</td>\n",
       "      <td>1.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-</td>\n",
       "      <td>Malicious</td>\n",
       "      <td>PartOfAHorizontalPortScan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.525880e+09</td>\n",
       "      <td>C3GBTkINvXNjVGtN5</td>\n",
       "      <td>192.168.100.103</td>\n",
       "      <td>41101.0</td>\n",
       "      <td>111.40.23.49</td>\n",
       "      <td>23.0</td>\n",
       "      <td>tcp</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>...</td>\n",
       "      <td>-</td>\n",
       "      <td>0.0</td>\n",
       "      <td>S</td>\n",
       "      <td>1.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-</td>\n",
       "      <td>Malicious</td>\n",
       "      <td>PartOfAHorizontalPortScan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.525880e+09</td>\n",
       "      <td>CDe43c1PtgynajGI6</td>\n",
       "      <td>192.168.100.103</td>\n",
       "      <td>60905.0</td>\n",
       "      <td>131.174.215.147</td>\n",
       "      <td>23.0</td>\n",
       "      <td>tcp</td>\n",
       "      <td>-</td>\n",
       "      <td>2.998796</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-</td>\n",
       "      <td>0.0</td>\n",
       "      <td>S</td>\n",
       "      <td>3.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-</td>\n",
       "      <td>Malicious</td>\n",
       "      <td>PartOfAHorizontalPortScan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.525880e+09</td>\n",
       "      <td>CJaDcG3MZzvf1YVYI4</td>\n",
       "      <td>192.168.100.103</td>\n",
       "      <td>44301.0</td>\n",
       "      <td>91.42.47.63</td>\n",
       "      <td>23.0</td>\n",
       "      <td>tcp</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>...</td>\n",
       "      <td>-</td>\n",
       "      <td>0.0</td>\n",
       "      <td>S</td>\n",
       "      <td>1.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-</td>\n",
       "      <td>Malicious</td>\n",
       "      <td>PartOfAHorizontalPortScan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             ts                 uid        id.orig_h  id.orig_p  \\\n",
       "0  1.525880e+09  CUmrqr4svHuSXJy5z7  192.168.100.103    51524.0   \n",
       "1  1.525880e+09  CH98aB3s1kJeq6SFOc  192.168.100.103    56305.0   \n",
       "2  1.525880e+09   C3GBTkINvXNjVGtN5  192.168.100.103    41101.0   \n",
       "3  1.525880e+09   CDe43c1PtgynajGI6  192.168.100.103    60905.0   \n",
       "4  1.525880e+09  CJaDcG3MZzvf1YVYI4  192.168.100.103    44301.0   \n",
       "\n",
       "         id.resp_h  id.resp_p proto service  duration orig_bytes  ...  \\\n",
       "0   65.127.233.163       23.0   tcp       -  2.999051          0  ...   \n",
       "1    63.150.16.171       23.0   tcp       -         -          -  ...   \n",
       "2     111.40.23.49       23.0   tcp       -         -          -  ...   \n",
       "3  131.174.215.147       23.0   tcp       -  2.998796          0  ...   \n",
       "4      91.42.47.63       23.0   tcp       -         -          -  ...   \n",
       "\n",
       "  local_resp missed_bytes history orig_pkts  orig_ip_bytes resp_pkts  \\\n",
       "0          -          0.0       S       3.0          180.0       0.0   \n",
       "1          -          0.0       S       1.0           60.0       0.0   \n",
       "2          -          0.0       S       1.0           60.0       0.0   \n",
       "3          -          0.0       S       3.0          180.0       0.0   \n",
       "4          -          0.0       S       1.0           60.0       0.0   \n",
       "\n",
       "   resp_ip_bytes  tunnel_parents      label             detailed-label  \n",
       "0            0.0               -  Malicious  PartOfAHorizontalPortScan  \n",
       "1            0.0               -  Malicious  PartOfAHorizontalPortScan  \n",
       "2            0.0               -  Malicious  PartOfAHorizontalPortScan  \n",
       "3            0.0               -  Malicious  PartOfAHorizontalPortScan  \n",
       "4            0.0               -  Malicious  PartOfAHorizontalPortScan  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- STATISTICAL SUMMARY ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ts</th>\n",
       "      <th>id.orig_p</th>\n",
       "      <th>id.resp_p</th>\n",
       "      <th>missed_bytes</th>\n",
       "      <th>orig_pkts</th>\n",
       "      <th>orig_ip_bytes</th>\n",
       "      <th>resp_pkts</th>\n",
       "      <th>resp_ip_bytes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.008748e+06</td>\n",
       "      <td>1.008748e+06</td>\n",
       "      <td>1.008748e+06</td>\n",
       "      <td>1008748.0</td>\n",
       "      <td>1.008748e+06</td>\n",
       "      <td>1.008748e+06</td>\n",
       "      <td>1.008748e+06</td>\n",
       "      <td>1.008748e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.526075e+09</td>\n",
       "      <td>4.443684e+04</td>\n",
       "      <td>1.609771e+04</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.496242e+00</td>\n",
       "      <td>8.114562e+01</td>\n",
       "      <td>1.424647e-01</td>\n",
       "      <td>9.049184e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.157431e+05</td>\n",
       "      <td>9.660592e+03</td>\n",
       "      <td>1.956280e+04</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.741176e+00</td>\n",
       "      <td>9.473090e+01</td>\n",
       "      <td>1.850414e+00</td>\n",
       "      <td>1.196776e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.525880e+09</td>\n",
       "      <td>3.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.525975e+09</td>\n",
       "      <td>4.373000e+04</td>\n",
       "      <td>2.300000e+01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>4.000000e+01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.526071e+09</td>\n",
       "      <td>4.376300e+04</td>\n",
       "      <td>8.080000e+03</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>6.000000e+01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.526174e+09</td>\n",
       "      <td>4.881400e+04</td>\n",
       "      <td>2.818025e+04</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>6.000000e+01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.526283e+09</td>\n",
       "      <td>6.539400e+04</td>\n",
       "      <td>6.553500e+04</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.000000e+01</td>\n",
       "      <td>2.990000e+03</td>\n",
       "      <td>7.500000e+01</td>\n",
       "      <td>9.415000e+03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 ts     id.orig_p     id.resp_p  missed_bytes     orig_pkts  \\\n",
       "count  1.008748e+06  1.008748e+06  1.008748e+06     1008748.0  1.008748e+06   \n",
       "mean   1.526075e+09  4.443684e+04  1.609771e+04           0.0  1.496242e+00   \n",
       "std    1.157431e+05  9.660592e+03  1.956280e+04           0.0  1.741176e+00   \n",
       "min    1.525880e+09  3.000000e+00  0.000000e+00           0.0  0.000000e+00   \n",
       "25%    1.525975e+09  4.373000e+04  2.300000e+01           0.0  1.000000e+00   \n",
       "50%    1.526071e+09  4.376300e+04  8.080000e+03           0.0  1.000000e+00   \n",
       "75%    1.526174e+09  4.881400e+04  2.818025e+04           0.0  1.000000e+00   \n",
       "max    1.526283e+09  6.539400e+04  6.553500e+04           0.0  6.000000e+01   \n",
       "\n",
       "       orig_ip_bytes     resp_pkts  resp_ip_bytes  \n",
       "count   1.008748e+06  1.008748e+06   1.008748e+06  \n",
       "mean    8.114562e+01  1.424647e-01   9.049184e+00  \n",
       "std     9.473090e+01  1.850414e+00   1.196776e+02  \n",
       "min     0.000000e+00  0.000000e+00   0.000000e+00  \n",
       "25%     4.000000e+01  0.000000e+00   0.000000e+00  \n",
       "50%     6.000000e+01  0.000000e+00   0.000000e+00  \n",
       "75%     6.000000e+01  0.000000e+00   0.000000e+00  \n",
       "max     2.990000e+03  7.500000e+01   9.415000e+03  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- MISSING VALUES ---\n",
      "No missing values found!\n",
      "\n",
      "--- CATEGORICAL COLUMNS ---\n",
      "\n",
      "uid:\n",
      "No missing values found!\n",
      "\n",
      "--- CATEGORICAL COLUMNS ---\n",
      "\n",
      "uid:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "uid\n",
       "CUmrqr4svHuSXJy5z7    1\n",
       "COj4Eq4lmR86amgfI6    1\n",
       "CBqL9l4KOG0Y3zauml    1\n",
       "CLIXVIOgCuf9Pv6j      1\n",
       "CImdWC3kB3Zhudx4q3    1\n",
       "CkA64v2h8KpX8Q74Z3    1\n",
       "CogrdgtK89aNUYYJd     1\n",
       "CaKHRA4f0t6bWYCo93    1\n",
       "CWtdXj3YoOcd5MAjm1    1\n",
       "C2lNJW3oXHusTkzmdg    1\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "id.orig_h:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "id.orig_h\n",
       "192.168.100.103    991061\n",
       "192.168.100.1        1651\n",
       "4.68.110.10            43\n",
       "194.70.98.42           23\n",
       "218.248.235.161        13\n",
       "83.168.243.156         12\n",
       "144.75.175.50          11\n",
       "38.104.45.226          11\n",
       "218.248.235.129        10\n",
       "159.226.254.70         10\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "id.resp_h:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "id.resp_h\n",
       "192.168.100.103    17687\n",
       "147.231.100.5       4313\n",
       "213.239.154.12      1428\n",
       "37.187.104.44       1408\n",
       "89.221.214.130      1402\n",
       "210.206.154.134      129\n",
       "70.45.29.240         128\n",
       "175.196.5.46         125\n",
       "92.255.209.3         125\n",
       "221.5.224.77         124\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "proto:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "proto\n",
       "tcp     583134\n",
       "udp     408193\n",
       "icmp     17421\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "service:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "service\n",
       "-       1005507\n",
       "http       3238\n",
       "dhcp          1\n",
       "ssh           1\n",
       "dns           1\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... and 10 more categorical columns (not shown)\n",
      "\n",
      "--- NUMERIC COLUMNS SUMMARY ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ts</th>\n",
       "      <th>id.orig_p</th>\n",
       "      <th>id.resp_p</th>\n",
       "      <th>missed_bytes</th>\n",
       "      <th>orig_pkts</th>\n",
       "      <th>orig_ip_bytes</th>\n",
       "      <th>resp_pkts</th>\n",
       "      <th>resp_ip_bytes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.008748e+06</td>\n",
       "      <td>1.008748e+06</td>\n",
       "      <td>1.008748e+06</td>\n",
       "      <td>1008748.0</td>\n",
       "      <td>1.008748e+06</td>\n",
       "      <td>1.008748e+06</td>\n",
       "      <td>1.008748e+06</td>\n",
       "      <td>1.008748e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.526075e+09</td>\n",
       "      <td>4.443684e+04</td>\n",
       "      <td>1.609771e+04</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.496242e+00</td>\n",
       "      <td>8.114562e+01</td>\n",
       "      <td>1.424647e-01</td>\n",
       "      <td>9.049184e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.157431e+05</td>\n",
       "      <td>9.660592e+03</td>\n",
       "      <td>1.956280e+04</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.741176e+00</td>\n",
       "      <td>9.473090e+01</td>\n",
       "      <td>1.850414e+00</td>\n",
       "      <td>1.196776e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.525880e+09</td>\n",
       "      <td>3.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.525975e+09</td>\n",
       "      <td>4.373000e+04</td>\n",
       "      <td>2.300000e+01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>4.000000e+01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.526071e+09</td>\n",
       "      <td>4.376300e+04</td>\n",
       "      <td>8.080000e+03</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>6.000000e+01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.526174e+09</td>\n",
       "      <td>4.881400e+04</td>\n",
       "      <td>2.818025e+04</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>6.000000e+01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.526283e+09</td>\n",
       "      <td>6.539400e+04</td>\n",
       "      <td>6.553500e+04</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.000000e+01</td>\n",
       "      <td>2.990000e+03</td>\n",
       "      <td>7.500000e+01</td>\n",
       "      <td>9.415000e+03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 ts     id.orig_p     id.resp_p  missed_bytes     orig_pkts  \\\n",
       "count  1.008748e+06  1.008748e+06  1.008748e+06     1008748.0  1.008748e+06   \n",
       "mean   1.526075e+09  4.443684e+04  1.609771e+04           0.0  1.496242e+00   \n",
       "std    1.157431e+05  9.660592e+03  1.956280e+04           0.0  1.741176e+00   \n",
       "min    1.525880e+09  3.000000e+00  0.000000e+00           0.0  0.000000e+00   \n",
       "25%    1.525975e+09  4.373000e+04  2.300000e+01           0.0  1.000000e+00   \n",
       "50%    1.526071e+09  4.376300e+04  8.080000e+03           0.0  1.000000e+00   \n",
       "75%    1.526174e+09  4.881400e+04  2.818025e+04           0.0  1.000000e+00   \n",
       "max    1.526283e+09  6.539400e+04  6.553500e+04           0.0  6.000000e+01   \n",
       "\n",
       "       orig_ip_bytes     resp_pkts  resp_ip_bytes  \n",
       "count   1.008748e+06  1.008748e+06   1.008748e+06  \n",
       "mean    8.114562e+01  1.424647e-01   9.049184e+00  \n",
       "std     9.473090e+01  1.850414e+00   1.196776e+02  \n",
       "min     0.000000e+00  0.000000e+00   0.000000e+00  \n",
       "25%     4.000000e+01  0.000000e+00   0.000000e+00  \n",
       "50%     6.000000e+01  0.000000e+00   0.000000e+00  \n",
       "75%     6.000000e+01  0.000000e+00   0.000000e+00  \n",
       "max     2.990000e+03  7.500000e+01   9.415000e+03  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Detailed analysis of a specific dataset\n",
    "def analyze_dataset(dataset_name):\n",
    "    \"\"\"\n",
    "    Perform detailed analysis on a specific dataset\n",
    "    \n",
    "    Args:\n",
    "        dataset_name: Key name of the dataset to analyze\n",
    "    \"\"\"\n",
    "    if dataset_name not in datasets:\n",
    "        print(f\"Dataset '{dataset_name}' not found.\")\n",
    "        print(f\"Available datasets: {list(datasets.keys())}\")\n",
    "        return\n",
    "    \n",
    "    df = datasets[dataset_name]\n",
    "    \n",
    "    # Basic information\n",
    "    print(f\"='='='='='='='='='='='='='='='='='='='='='='='='='='='='='='='='='='='='='='='='=\")\n",
    "    print(f\"DATASET: {dataset_name}\")\n",
    "    print(f\"='='='='='='='='='='='='='='='='='='='='='='='='='='='='='='='='='='='='='='='='=\")\n",
    "    print(f\"Shape: {df.shape[0]} rows x {df.shape[1]} columns\")\n",
    "    \n",
    "    # Column information\n",
    "    print(\"\\n--- COLUMNS ---\")\n",
    "    for col in df.columns:\n",
    "        print(f\"- {col}: {df[col].dtype}\")\n",
    "    \n",
    "    # Sample data\n",
    "    print(\"\\n--- SAMPLE DATA ---\")\n",
    "    display(df.head())\n",
    "    \n",
    "    # Statistical summary\n",
    "    print(\"\\n--- STATISTICAL SUMMARY ---\")\n",
    "    display(df.describe())\n",
    "    \n",
    "    # Missing values\n",
    "    print(\"\\n--- MISSING VALUES ---\")\n",
    "    missing = df.isna().sum()\n",
    "    if missing.sum() > 0:\n",
    "        display(pd.DataFrame({\n",
    "            'Column': missing.index,\n",
    "            'Missing Values': missing.values,\n",
    "            'Percentage': (missing.values / len(df) * 100).round(2)\n",
    "        })[missing.values > 0].sort_values('Missing Values', ascending=False))\n",
    "    else:\n",
    "        print(\"No missing values found!\")\n",
    "    \n",
    "    # Value counts for categorical columns (showing top categories)\n",
    "    print(\"\\n--- CATEGORICAL COLUMNS ---\")\n",
    "    cat_columns = df.select_dtypes(include=['object', 'category']).columns\n",
    "    for col in cat_columns[:5]:  # Limiting to first 5 categorical columns\n",
    "        print(f\"\\n{col}:\")\n",
    "        display(df[col].value_counts().head(10))\n",
    "    \n",
    "    if len(cat_columns) > 5:\n",
    "        print(f\"... and {len(cat_columns) - 5} more categorical columns (not shown)\")\n",
    "    \n",
    "    # Numeric column distributions\n",
    "    print(\"\\n--- NUMERIC COLUMNS SUMMARY ---\")\n",
    "    num_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "    if len(num_cols) > 0:\n",
    "        display(df[num_cols].describe())\n",
    "    else:\n",
    "        print(\"No numeric columns found!\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# List available datasets\n",
    "print(\"Available datasets:\")\n",
    "for i, key in enumerate(datasets.keys()):\n",
    "    print(f\"{i+1}. {key}\")\n",
    "\n",
    "# Example: Analyze the first dataset (you can change the index as needed)\n",
    "if datasets:\n",
    "    dataset_name = list(datasets.keys())[0]  # First dataset\n",
    "    print(f\"\\nAnalyzing first dataset: {dataset_name}\")\n",
    "    analyze_dataset(dataset_name)\n",
    "else:\n",
    "    print(\"No datasets available to analyze.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlproject1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
